{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygraphblas import *\n",
    "#from pygraphblas.demo.gviz import draw, draw_op\n",
    "import pygraphblas.descriptor\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from CSV format\n",
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def load_node(self, filename):\n",
    "        filename = self.path + filename\n",
    "        with open(filename, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "            original_ids = [row['id:ID'] for row in reader]\n",
    "            id_mapping = {}\n",
    "            for index in range(len(original_ids)):\n",
    "                id_mapping[original_ids[index]] = index\n",
    "            \n",
    "        return original_ids, id_mapping\n",
    "\n",
    "    def load_edge(self, filename, start_mapping, end_mapping, typ=BOOL, drop_dangling_edges=False):\n",
    "        filename = self.path + filename\n",
    "        with open(filename, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "            row_ids = []\n",
    "            col_ids = []\n",
    "            values = []\n",
    "            for row in reader:\n",
    "                start_id = row['id:START_ID']\n",
    "                end_id = row['id:END_ID']\n",
    "                if not drop_dangling_edges or (start_id in start_mapping and end_id in end_mapping):\n",
    "                    row_ids.append(start_mapping[start_id])\n",
    "                    col_ids.append(end_mapping[end_id])\n",
    "                    values.append(1)\n",
    "        \n",
    "            edge_matrix = Matrix.from_lists(\n",
    "            row_ids,\n",
    "            col_ids,\n",
    "            values,\n",
    "            nrows=len(start_mapping), \n",
    "            ncols=len(end_mapping), \n",
    "            typ=typ)\n",
    "            return edge_matrix\n",
    "\n",
    "def print_data_dimensions(vertices, matrices):\n",
    "    for vertex in vertices:\n",
    "        print(f\"dimension of {vertex} is {len(vertices[vertex])}\")\n",
    "    \n",
    "    for matrix in matrices:\n",
    "        print(f\"dimension of {matrix} is {matrices[matrix].shape}\")\n",
    "\n",
    "def get_ids(matrix, key, vertices):\n",
    "    I, J, V = matrix.to_lists()\n",
    "    ids = [vertices[key][i] for i in I]\n",
    "    print(ids)\n",
    "    print(f'Total number of ids for key {key} in mx: {len(matrix)}')\n",
    "    print('---------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['67']\n",
      "Total number of ids for key Semaphore in mx: 1\n",
      "---------------------------------------------------\n",
      "    0 1 2 3 4\n",
      " 0|          | 0\n",
      " 1|          | 1\n",
      " 2|          | 2\n",
      " 3|       1  | 3\n",
      " 4|          | 4\n",
      "    0 1 2 3 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Matrix (5x5 : 1:BOOL)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Check results here:\n",
    "https://github.com/ftsrg/trainbenchmark/blob/master/trainbenchmark-tool/src/main/java/hu/bme/mit/trainbenchmark/benchmark/test/TrainBenchmarkTest.java#L139 \n",
    "'''\n",
    "\n",
    "path = 'trainbenchmark-repair-models-csv/'\n",
    "loader = DataLoader(path)\n",
    "data_size = 1\n",
    "\n",
    "seg_df = pd.read_csv(path+f'railway-repair-{data_size}-Segment.csv')\n",
    "sw_df = pd.read_csv(path+f'railway-repair-{data_size}-Switch.csv')\n",
    "te_df = pd.concat([seg_df, sw_df])\n",
    "te_df.to_csv(path+'trackelement.csv', index=False)\n",
    "\n",
    "vertices = {}\n",
    "mapping = {}\n",
    "vertices['Route'], mapping['Route'] = loader.load_node(f'railway-repair-{data_size}-Route.csv')\n",
    "vertices['SwitchPosition'], mapping['SwitchPosition'] = loader.load_node(f'railway-repair-{data_size}-SwitchPosition.csv')\n",
    "vertices['Switch'], mapping['Switch'] = loader.load_node(f'railway-repair-{data_size}-Switch.csv')\n",
    "vertices['Sensor'], mapping['Sensor'] = loader.load_node(f'railway-repair-{data_size}-Sensor.csv')\n",
    "vertices['Segment'], mapping['Segment'] = loader.load_node(f'railway-repair-{data_size}-Segment.csv')\n",
    "vertices['Semaphore'], mapping['Semaphore'] = loader.load_node(f'railway-repair-{data_size}-Semaphore.csv')\n",
    "vertices['TrackElement'], mapping['TrackElement'] = loader.load_node('trackelement.csv')\n",
    "\n",
    "matrices = {}\n",
    "matrices['follows'] = loader.load_edge(f'railway-repair-{data_size}-follows.csv', mapping['Route'], mapping['SwitchPosition'])\n",
    "matrices['target'] = loader.load_edge(f'railway-repair-{data_size}-target.csv', mapping['SwitchPosition'], mapping['Switch'])\n",
    "matrices['monitoredBySwitch'] = loader.load_edge(f'railway-repair-{data_size}-monitoredBy.csv', mapping['Switch'], mapping['Sensor'], drop_dangling_edges=True)\n",
    "matrices['monitoredBySegment'] = loader.load_edge(f'railway-repair-{data_size}-monitoredBy.csv', mapping['Segment'], mapping['Sensor'], drop_dangling_edges=True)\n",
    "\n",
    "\n",
    "matrices['monitoredBy'] = loader.load_edge(f'railway-repair-{data_size}-monitoredBy.csv', mapping['TrackElement'], mapping['Sensor'], drop_dangling_edges=False)\n",
    "\n",
    "matrices['requires'] = loader.load_edge(f'railway-repair-{data_size}-requires.csv', mapping['Route'], mapping['Sensor'])\n",
    "matrices['connectsToSeg'] = loader.load_edge(f'railway-repair-{data_size}-connectsTo.csv', mapping['Segment'], mapping['Segment'], drop_dangling_edges=True)\n",
    "matrices['connectsToTrackElem'] = loader.load_edge(f'railway-repair-{data_size}-connectsTo.csv', mapping['TrackElement'], mapping['TrackElement'], drop_dangling_edges=False)\n",
    "matrices['exit'] = loader.load_edge(f'railway-repair-{data_size}-exit.csv', mapping['Route'], mapping['Semaphore'])\n",
    "matrices['entry'] = loader.load_edge(f'railway-repair-{data_size}-entry.csv', mapping['Route'], mapping['Semaphore'])\n",
    "\n",
    "#print_data_dimensions(vertices, matrices)\n",
    "\n",
    "#Uncomment to trace down specific Sensor\n",
    "#selected_sensor_id = mapping['Sensor']['1692']\n",
    "\n",
    "def route_sensor_violation_query(matrices):\n",
    "    route_to_switch = matrices['follows'] @ matrices['target']\n",
    "    route_to_sensor = route_to_switch @ matrices['monitoredBySwitch']\n",
    "    return route_to_sensor.extract_matrix(matrices['requires'], desc=descriptor.ooco)\n",
    "\n",
    "\n",
    "def connected_segments_query(matrices, vertices):\n",
    "    monitoredBySegmentTransposed = matrices['monitoredBySegment'].transpose()\n",
    "    res = monitoredBySegmentTransposed.dup()\n",
    "    for _ in range(5):\n",
    "        #I, J, V = res[[selected_sensor_id],:].to_lists()\n",
    "        #print([vertices['Segment'][j] for j in J])\n",
    "        res = res.mxm(matrices['connectsToSeg'], mask=monitoredBySegmentTransposed)\n",
    "        \n",
    "    I, J, V = res.to_lists()\n",
    "    violating_sensor_ids = [vertices['Sensor'][i] for i in I]\n",
    "    print('The IDs of the sensors that violate the constraints:')\n",
    "    print(violating_sensor_ids)\n",
    "    print(f'The total number of violations is {len(res)}')\n",
    "    get_ids(res, 'Sensor', vertices)\n",
    "    \n",
    "\n",
    "def semaphore_neighbor_query(matrices, vertices):    \n",
    "    monitoredByTransposed = matrices['monitoredBy'].transpose()\n",
    "    connectsToTrackElemTransposed = matrices['connectsToTrackElem'].transpose()\n",
    "    requiresTransposed = matrices['requires'].transpose()\n",
    "    entryTransposed = matrices['entry'].transpose()\n",
    "\n",
    "    route_to_trackelement1 = matrices['requires'] @ monitoredByTransposed\n",
    "    route_to_trackelement2 = route_to_trackelement1 @ connectsToTrackElemTransposed\n",
    "    route_to_sensor = route_to_trackelement2 @ matrices['monitoredBy']\n",
    "    route_to_route = route_to_sensor @ requiresTransposed\n",
    "    route_to_route_off_diag = route_to_route.offdiag()\n",
    "    route_to_semaphore = route_to_route_off_diag @ matrices['exit']\n",
    "    res = route_to_semaphore.extract_matrix(mask=entryTransposed, desc=descriptor.ooco)\n",
    "    \n",
    "    \n",
    "    get_ids(res, 'Semaphore', vertices)\n",
    "    print(res.to_string())\n",
    "    \n",
    "    return res\n",
    "\n",
    "semaphore_neighbor_query(matrices, vertices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
